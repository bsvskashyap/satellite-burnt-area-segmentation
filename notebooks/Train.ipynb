{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70396791-216b-4f54-bc65-d6694441d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, UpSampling2D, LeakyReLU,BatchNormalization, Add,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from segmentation_models.metrics import IOUScore\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryCrossentropy,SparseCategoricalCrossentropy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67675fb-91ee-417f-9483-cbd6b7eb86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683d048-b449-4377-bea1-a2d52ed5b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = \"data/data_for_keras_aug/train_images/train/\"\n",
    "train_mask_dir = \"data/data_for_keras_aug/train_masks/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a94281-e0b4-4b9e-9459-0388e4ee8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = os.listdir(train_img_dir)\n",
    "msk_list = os.listdir(train_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd29781-f676-46e9-9787-da3c0ae87f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = len(os.listdir(train_img_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bd367-177b-493e-a68a-3e7884bf1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = random.randint(0, num_images-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4de93e-a204-4ba2-88b5-7502e8a6a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_for_plot = cv2.imread(train_img_dir+img_list[img_num], 1)\n",
    "img_for_plot = cv2.cvtColor(img_for_plot, cv2.COLOR_BGR2RGB)\n",
    "mask_for_plot =cv2.imread(train_mask_dir+msk_list[img_num], 0)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_for_plot)\n",
    "plt.title('Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_for_plot, cmap='gray')\n",
    "plt.title('Mask')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa96a8-91b1-499d-88de-9027170451a5",
   "metadata": {},
   "source": [
    "## Defining seed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b82a4-50b5-4c56-93b2-8947c50bcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=24\n",
    "batch_size= 8\n",
    "n_classes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae4948-caad-4420-9023-318f283015c4",
   "metadata": {},
   "source": [
    "### BackBone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006665fc-8372-4672-9e3e-6ba832970b05",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd7309-c1f0-41a7-88ca-2c8f0bfb9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this to preprocess input for transfer learning\n",
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f718be-ad0f-4bea-9251-e66168511243",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "def preprocess_data(img, mask, num_class):\n",
    "    #Scale images\n",
    "    img = scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n",
    "    img = preprocess_input(img)  #Preprocess based on the pretrained backbone...\n",
    "    #Convert mask to one-hot\n",
    "    mask = mask / 255.0  # Normalize mask values to [0, 1]\n",
    "    mask = to_categorical(mask, num_class)\n",
    "      \n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005b87a-48e2-48fa-8e25-3e7c345d1c55",
   "metadata": {},
   "source": [
    "## Train Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf7198-6895-4334-9dfa-59d77a8a9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95cf52-1d40-47a2-bc71-d60327bbed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler  # Assuming StandardScaler is used\n",
    "\n",
    "def trainGenerator(train_img_path, train_mask_path, num_class):\n",
    "    img_data_gen_args = dict(\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rotation_range=30,           # Random rotations\n",
    "        width_shift_range=0.1,       # Random width shifts\n",
    "        height_shift_range=0.1,      # Random height shifts\n",
    "        zoom_range=0.2,              # Random zoom\n",
    "        fill_mode='reflect'\n",
    "    )\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**img_data_gen_args)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_img_path,\n",
    "        target_size=(256, 256),  # Adjust based on your model input size\n",
    "        color_mode='rgb',        # Assuming RGB images\n",
    "        class_mode=None,\n",
    "        batch_size=batch_size,\n",
    "        seed=seed)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_mask_path,\n",
    "        target_size=(256, 256),  # Adjust based on your model input size\n",
    "        color_mode='grayscale',\n",
    "        class_mode=None,\n",
    "        batch_size=batch_size,\n",
    "        seed=seed)\n",
    "    \n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    \n",
    "    for (img, mask) in train_generator:\n",
    "        img, mask = preprocess_data(img, mask, num_class)\n",
    "        yield img, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795eaeb3-44ae-4fbd-89a7-1be1e54f8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = \"data/data_for_keras_aug/train_images/\"\n",
    "train_mask_path = \"data/data_for_keras_aug/train_masks/\"\n",
    "train_img_gen = trainGenerator(train_img_path, train_mask_path, num_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca750ef6-f3d9-41bb-80a2-a4a39271a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_img_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae6bac-ce6b-425c-8a91-a39425924d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,3):\n",
    "    image = x[i]\n",
    "    mask = np.argmax(y[i], axis=2)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7994a21-1ca3-4ba2-bf21-98af3588adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_path = \"data/data_for_keras_aug/val_images/\"\n",
    "val_mask_path = \"data/data_for_keras_aug/val_masks/\"\n",
    "val_img_gen = trainGenerator(val_img_path, val_mask_path, num_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544395b-bf16-4980-872a-052e814156bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = val_img_gen.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c24d7-c4b8-41a5-b2d9-46b043cfbe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    image = x_val[i]\n",
    "    mask = np.argmax(y_val[i], axis=2)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b23b5-711c-4e9f-ac0f-591897633539",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_imgs = len(os.listdir('data/data_for_keras_aug/train_images/train/'))\n",
    "num_val_images = len(os.listdir('data/data_for_keras_aug/val_images/val/'))\n",
    "steps_per_epoch = num_train_imgs//batch_size\n",
    "val_steps_per_epoch = num_val_images//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee3066-313c-4063-a321-12c8119afd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = x.shape[1]\n",
    "IMG_WIDTH  = x.shape[2]\n",
    "IMG_CHANNELS = x.shape[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910f96f-84f1-4d2b-a910-8ababc64c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fba25c-1abd-42b6-9283-c6a7b3ea5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d976372-9c56-42c0-9dd5-d3a025913284",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b7153-c4cc-46d8-baf0-8d534ebdaa4a",
   "metadata": {},
   "source": [
    "### Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d05bf-4cf3-481a-9430-f99e0b26e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DiceLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        intersection = tf.reduce_sum(y_true * y_pred)\n",
    "        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "        dice = (2.0 * intersection + 1e-7) / (union + 1e-7)\n",
    "        return 1.0 - dice  \n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    y_true_float = tf.cast(y_true, tf.float32)\n",
    "    dice_loss = DiceLoss()(y_true_float, y_pred)\n",
    "    l2_reg = 0.01  \n",
    "    l2_loss = l2_reg * tf.reduce_sum([tf.reduce_mean(tf.square(w)) for w in tf.compat.v1.trainable_variables()])\n",
    "    focal_loss_value = sm.losses.categorical_focal_loss(y_true, y_pred)\n",
    "    combined_loss = dice_loss + l2_loss\n",
    "    \n",
    "    return combined_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292d9d1e-c077-46af-8f22-e25b3dde5ce4",
   "metadata": {},
   "source": [
    "### Total Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa225e-e8d9-4f0e-8840-ec12ed397fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dice_loss = sm.losses.DiceLoss() \n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2dd05-2c64-4a29-8922-fa4d00ed93bf",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f6829-d0fe-481d-b789-2b55850b6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models.losses import DiceLoss\n",
    "from segmentation_models.metrics import IOUScore\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model = sm.Unet(BACKBONE, encoder_weights='imagenet', \n",
    "                input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "                classes=2, activation='softmax')\n",
    "\n",
    "##Try playing around with different loss functions\n",
    "model.compile(optimizer=Adam(1e-4), loss= DiceLoss(),  metrics=['accuracy', IOUScore()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633a97d-97bb-4578-b054-8712b0306498",
   "metadata": {},
   "source": [
    "### FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8484fe1-5820-42bb-8264-c1b036259fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50  # Substitute with ResNet34 \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "### This is an implementation of FCN (Fully Convolutional Networks) \n",
    "### You can see slight changes in the architectures of this and the Unet in the next code block.\n",
    "\n",
    "def build_fcn_32s(input_shape, num_classes):\n",
    "    resnet = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = False  # Freeze ResNet layers\n",
    "\n",
    "    # Get the output of the ResNet encoder (last pooling layer output)\n",
    "    encoder_output = resnet.output  # Shape should be (8, 8, 2048) for ResNet50\n",
    "\n",
    "    # Convolution layer for class predictions\n",
    "    x = Conv2D(512, (7, 7), activation='relu', padding='same')(encoder_output)\n",
    "    x = Conv2D(512, (1, 1), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
    "\n",
    "    upsampled_output = UpSampling2D(size=(32, 32), interpolation='bilinear')(x)\n",
    "\n",
    "    \n",
    "    model = Model(inputs=resnet.input, outputs=upsampled_output)\n",
    "    return model\n",
    "\n",
    "# Set parameters\n",
    "input_shape = (256, 256, 3)  \n",
    "num_classes = 2 \n",
    "\n",
    "# Build and compile the model\n",
    "model = build_fcn_32s(input_shape, num_classes)\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f80e9-e89a-4b6b-9c3e-8a5ca1f3707f",
   "metadata": {},
   "source": [
    "### Custom UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe00b88-d62b-4590-a356-ed8df2d21a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def multi_unet_model(n_classes=2, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=3):\n",
    "    # Build the model\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    s = inputs\n",
    "\n",
    "    # Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.2)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.2)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    # Expansive path\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.2)(c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.2)(c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    ### This is a learning rate scheduler, it decays the learning rate as the model learns, we can set the number of steps after which the learning rate should start decaying and the factor by which it decays.\n",
    "    ### Change the decay steps based on the number of epochs. May be you can try number of epochs/10 as the decay steps, so for 25 may be give 2 steps per decay and try increasing fromm there.\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=2,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    ##try to play with loss functions,hyper parameters like number of filters,optimizer learning rates.\n",
    "    model.compile(optimizer=Adam(), loss=sm.losses.cce_jaccard_loss, metrics=['accuracy', IOUScore()])\n",
    "    ### model.summary gives you the idea of the components of the model and its architecture. You can get a better understanding of the models config.\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = multi_unet_model(n_classes=2, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e87926-6e61-49de-80a0-44fffa31df8d",
   "metadata": {},
   "source": [
    "### Unet + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3bdee-b5af-4836-8619-e23329d6d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, UpSampling2D, LSTM, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from segmentation_models.metrics import IOUScore\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "def conv_block(inputs, filters, kernel_size=3, activation='relu', padding='same'):\n",
    "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(inputs)\n",
    "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv)\n",
    "    return conv\n",
    "\n",
    "def build_unet_with_lstm(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = conv_block(inputs, 64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = conv_block(pool1, 128)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = conv_block(pool2, 256)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = conv_block(pool3, 512)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = conv_block(pool4, 1024)\n",
    "    \n",
    "    # Reshape before LSTM\n",
    "    lstm_input_shape = (16*16, 1024)  # Flattened to treat as a sequence\n",
    "    lstm = Reshape(lstm_input_shape)(conv5)\n",
    "    \n",
    "    # LSTM layer\n",
    "    lstm = LSTM(512, return_sequences=True)(lstm)\n",
    "    \n",
    "    # Reshape after LSTM\n",
    "    lstm_output_shape = (16, 16, 512)  # Reshape back to original shape\n",
    "    lstm = Reshape(lstm_output_shape)(lstm)\n",
    "    \n",
    "    # Decoder\n",
    "    up6 = concatenate([UpSampling2D(size=(2, 2))(lstm), conv4], axis=-1)\n",
    "    conv6 = conv_block(up6, 512)\n",
    "    \n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
    "    conv7 = conv_block(up7, 256)\n",
    "    \n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
    "    conv8 = conv_block(up8, 128)\n",
    "    \n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
    "    conv9 = conv_block(up9, 64)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 3e-4\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "model_with_lstm = build_unet_with_lstm(input_shape=(256, 256, 3), num_classes=2)\n",
    "model_with_lstm.compile(optimizer=Adam(), loss=DiceLoss(), metrics=['accuracy', IOUScore()])\n",
    "\n",
    "model_with_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac041b8e-2db5-4f22-89ec-f366c1e04eda",
   "metadata": {},
   "source": [
    "### Unet +LSTM and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba3f08-cce5-423e-a432-9a35ce1c4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, UpSampling2D, LSTM, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from segmentation_models.metrics import IOUScore\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from segmentation_models.losses import DiceLoss\n",
    "from segmentation_models.metrics import FScore\n",
    "\n",
    "def conv_block(inputs, filters, kernel_size=3, activation='relu', padding='same'):\n",
    "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(inputs)\n",
    "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv)\n",
    "    conv = Dropout(0.2)(conv)  # Adding dropout layer with a dropout rate of 20%\n",
    "    return conv\n",
    "\n",
    "def build_unet_with_lstm(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = conv_block(inputs, 64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = conv_block(pool1, 128)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = conv_block(pool2, 256)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = conv_block(pool3, 512)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = conv_block(pool4, 1024)\n",
    "    \n",
    "    # Reshape before LSTM\n",
    "    lstm_input_shape = (16*16, 1024)  # Flattened to treat as a sequence\n",
    "    lstm = Reshape(lstm_input_shape)(conv5)\n",
    "    \n",
    "    # LSTM layer\n",
    "    lstm = LSTM(512, return_sequences=True)(lstm)\n",
    "    \n",
    "    # Reshape after LSTM\n",
    "    lstm_output_shape = (16, 16, 512)  # Reshape back to original shape\n",
    "    lstm = Reshape(lstm_output_shape)(lstm)\n",
    "    \n",
    "    # Decoder\n",
    "    up6 = concatenate([UpSampling2D(size=(2, 2))(lstm), conv4], axis=-1)\n",
    "    conv6 = conv_block(up6, 512)\n",
    "    \n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
    "    conv7 = conv_block(up7, 256)\n",
    "    \n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
    "    conv8 = conv_block(up8, 128)\n",
    "    \n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
    "    conv9 = conv_block(up9, 64)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=3e-4,\n",
    "    decay_steps=200,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "model_with_lstm = build_unet_with_lstm(input_shape=(256, 256, 3), num_classes=2)\n",
    "dice_metric = FScore(name='dice_coefficient', beta=1)\n",
    "\n",
    "model_with_lstm.compile(optimizer=Adam(1e-4), loss=\"categorical_crossentropy\",   metrics=['accuracy', IOUScore(), dice_metric])\n",
    "\n",
    "model_with_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ca552d-dfa3-4a06-8f73-2e8f9333079c",
   "metadata": {},
   "source": [
    "### Unet + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02e94d-0c05-4e00-a753-35790956f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, UpSampling2D, GRU, Reshape, Dropout, LayerNormalization,Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from segmentation_models.metrics import IOUScore\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from segmentation_models.losses import DiceLoss\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "\n",
    "def conv_block(inputs, filters, kernel_size=3, activation='leaky_relu', padding='same'):\n",
    "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(inputs)\n",
    "    conv = Conv2D(filters, kernel_size, activation=activation, padding=padding)(conv)\n",
    "    \n",
    "    conv = Dropout(0.2)(conv)  # Adding dropout layer with a dropout rate of 20%\n",
    "    return conv\n",
    "\n",
    "\n",
    "def build_unet_with_gru(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = conv_block(inputs, 64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = conv_block(pool1, 128)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = conv_block(pool2, 256)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = conv_block(pool3, 512)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = conv_block(pool4, 1024)\n",
    "    \n",
    "    # Reshape before GRU\n",
    "    gru_input_shape = (16*16, 1024)  # Flattened to treat as a sequence\n",
    "    gru = Reshape(gru_input_shape)(conv5)\n",
    "    \n",
    "    # GRU layer\n",
    "    gru = GRU(512, return_sequences=True)(gru)\n",
    "    \n",
    "    # Reshape after GRU\n",
    "    gru_output_shape = (16, 16, 512)  # Reshape back to original shape\n",
    "    gru = Reshape(gru_output_shape)(gru)\n",
    "    \n",
    "    # Decoder\n",
    "    up6 = concatenate([UpSampling2D(size=(2, 2))(gru), conv4], axis=-1)\n",
    "    conv6 = conv_block(up6, 512)\n",
    "    \n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=-1)\n",
    "    conv7 = conv_block(up7, 256)\n",
    "    \n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=-1)\n",
    "    conv8 = conv_block(up8, 128)\n",
    "    \n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=-1)\n",
    "    conv9 = conv_block(up9, 64)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Conv2D(num_classes, 1, activation='softmax')(conv9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=10,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "model_with_gru = build_unet_with_gru(input_shape=(256, 256, 3), num_classes=2)\n",
    "\n",
    "model_with_gru.compile(optimizer=Adam(1e-4), loss=\"categorical_crossentropy\", metrics=['accuracy', IOUScore()])\n",
    "\n",
    "model_with_gru.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983cc62-9f09-4127-95b8-822875b250e9",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354bc5c3-3fbe-442e-b812-afa1cd3fdb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Checkpoint to save the best model based on val_loss\n",
    "checkpoint = ModelCheckpoint('test.hdf5',\n",
    "                             monitor='val_loss',  # Track validation loss\n",
    "                             save_best_only=True,  \n",
    "                             mode='min',  # Minimize validation loss\n",
    "                             verbose=1)\n",
    "### try changing the epochs\n",
    "callbacks = [checkpoint]\n",
    "history=model.fit(train_img_gen,\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=25,\n",
    "          verbose=1,\n",
    "          validation_data=val_img_gen,\n",
    "          validation_steps=val_steps_per_epoch,\n",
    "          callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ad5dc-74d1-4d34-bdba-b5d7e57cfa77",
   "metadata": {},
   "source": [
    "### Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b7d31-5322-4229-a9e3-ef6a3a4c7beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22467e7c-d9c9-457c-8b0f-ead58c958648",
   "metadata": {},
   "source": [
    "### IOU Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e4fb0-22e6-4b89-8fcd-69286cec2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['iou_score']\n",
    "val_acc = history.history['val_iou_score']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training IoU')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation IoU')\n",
    "plt.title('Training and validation IoU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a3194-424d-4ccb-b4a2-f620981de632",
   "metadata": {},
   "source": [
    "### Accuracy Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fce2f8-404d-46d0-acdb-125007214ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, 'y', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8fc6f4-6531-4081-9846-a8e489205c71",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0374f-b0eb-41c2-8cec-c61f771e2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "### give your model's name here\n",
    "model = load_model(\"model.hdf5\", compile=False)\n",
    "\n",
    "\n",
    "test_image_batch, test_mask_batch = val_img_gen.__next__()\n",
    "\n",
    "\n",
    "test_mask_batch_argmax = np.argmax(test_mask_batch, axis=3)\n",
    "test_pred_batch = model.predict(test_image_batch)\n",
    "test_pred_batch_argmax = np.argmax(test_pred_batch, axis=3)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "accuracy.update_state(test_mask_batch_argmax, test_pred_batch_argmax)\n",
    "print(\"Accuracy =\", accuracy.result().numpy())\n",
    "\n",
    "n_classes = 2\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(test_pred_batch_argmax, test_mask_batch_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "# Flatten the arrays for classification report and confusion matrix\n",
    "test_mask_flat = test_mask_batch_argmax.flatten()\n",
    "test_pred_flat = test_pred_batch_argmax.flatten()\n",
    "\n",
    "# Calculate and print the classification report\n",
    "class_report = classification_report(test_mask_flat, test_pred_flat)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# Calculate and print the confusion matrix\n",
    "conf_matrix = confusion_matrix(test_mask_flat, test_pred_flat)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e75f2b-91be-48d4-a84b-fca89205169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, test_image_batch.shape[0]-1):\n",
    "    img_num=x\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(231)\n",
    "    plt.title('Testing Image')\n",
    "    plt.imshow(test_image_batch[img_num])\n",
    "    \n",
    "    plt.subplot(232)\n",
    "    plt.title('Testing Label')\n",
    "\n",
    "    plt.imshow(test_mask_batch_argmax[img_num],cmap='gray')\n",
    "    plt.subplot(233)\n",
    "    plt.title('Prediction on test image')\n",
    "    plt.imshow(test_pred_batch_argmax[img_num],cmap='magma')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
